# Student CLAP Configuration

# Audio processing (MUST match CLAP preprocessing)
audio:
  sample_rate: 48000           # Sample rate for audio loading
  segment_length: 480000       # 10 seconds at 48kHz (CRITICAL: match teacher)
  hop_length: 240000           # 5 seconds hop, 50% overlap (CRITICAL: match teacher)
  
  # Mel-spectrogram parameters (for student model input)
  n_mels: 128                  # Number of mel bands
  n_fft: 2048                  # FFT window size
  hop_length_stft: 480         # STFT hop length
  fmin: 0                      # Minimum frequency
  fmax: 14000                  # Maximum frequency

# Model architecture (TinyCLAP-inspired)
model:
  embedding_dim: 512           # MUST be 512 for CLAP compatibility
  
  # CNN stem
  cnn_channels: [32, 64, 128]
  
  # Transformer
  transformer_layers: 2
  attention_heads: 4
  hidden_dim: 256
  
  dropout: 0.1

# Training hyperparameters
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 0.0001
  epochs: 15
  
  optimizer: "adam"
  weight_decay: 0.00001
  grad_clip: 1.0
  
  # Loss weights
  mse_weight: 0.6
  cosine_weight: 0.4
  
  # Checkpointing
  save_every: 1                # Save checkpoint every epoch
  early_stopping_patience: 10  # Stop if no improvement for N epochs
  
  # Validation
  validation_split: 0.15       # 15% of data for validation

# Paths
paths:
  teacher_model: "../model/clap_audio_model.onnx"  # INPUT: CLAP teacher model (~268MB)
  mel_cache: "/Volumes/audiomuse/student_clap_cache/mel_spectrograms.db"  # SQLite DB (mel specs + embeddings)
  checkpoints: "./checkpoints"
  logs: "./logs"
  final_model: "./models/student_clap_audio.onnx"  # OUTPUT: trained student model (~11MB)

# Dataset configuration - LOCAL FMA
dataset:
  fma_path: "/Users/guidocolangiuli/Music/FMA"  # Path to local FMA audio files
  sample_size: 0  # Total number of songs to use (null = all songs)

# Logging
logging:
  level: "INFO"
  log_every: 10
