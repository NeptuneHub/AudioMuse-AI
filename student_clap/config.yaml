# Student CLAP Configuration

# Audio processing (MUST match CLAP preprocessing)
audio:
  sample_rate: 48000           # Sample rate for audio loading
  segment_length: 480000       # 10 seconds at 48kHz (CRITICAL: match teacher)
  hop_length: 240000           # 5 seconds hop, 50% overlap (CRITICAL: match teacher)
  
  # Mel-spectrogram parameters (for student model input)
  n_mels: 128                  # Number of mel bands
  n_fft: 2048                  # FFT window size
  hop_length_stft: 480         # STFT hop length
  fmin: 0                      # Minimum frequency
  fmax: 14000                  # Maximum frequency

# Model architecture (TinyCLAP-inspired)
model:
  embedding_dim: 512           # MUST be 512 for CLAP compatibility
  
  # CNN stem
  cnn_channels: [32, 64, 128]
  
  # Transformer
  transformer_layers: 2
  attention_heads: 4
  hidden_dim: 256
  
  dropout: 0.1

# Training hyperparameters
training:
  batch_size: 2                # Reduced from 4 to avoid swap (2x slower but stays in RAM)
  gradient_accumulation_steps: 8  # Increased to keep effective batch=16
  learning_rate: 0.003         # TinyCLAP Stage 1 LR (3e-3)
  epochs: 15
  
  # Two-stage training (like tinyCLAP)
  # Stage 1: Train entire model for 'epochs' epochs with lr=0.003
  # Stage 2: Freeze encoder, train only projection for 'stage2_epochs' epochs with lower LR
  stage2_epochs: 5           # Additional epochs for projection-only refinement
  stage2_learning_rate: 0.001  # TinyCLAP Stage 2 LR (1e-3) - 3x lower for fine-tuning
  projection_only: false     # Set to true for stage 2 (handled automatically)
  
  optimizer: "adam"
  weight_decay: 0.0          # No regularization - faster learning for distillation
  grad_clip: 5.0             # Higher clip for lr=0.003 (was 1.0 for old lr=0.0001)
  
  # Training strategy: "segments", "averaged", or "both"
  # - segments: Train on each segment individually (more data, segment-level learning)
  # - averaged: Train on averaged embedding per song (matches teacher CLAP - RECOMMENDED)
  # - both: Train on segments AND averaged (BEST: learns both individual + averaged patterns)
  training_strategy: "both"  # FIXED: Now uses single forward pass like devel branch!
  
  # Checkpointing
  save_every: 1                # Save checkpoint every epoch
  early_stopping_patience: 10  # Stop if no improvement for N epochs
  
  # Validation
  validation_split: 0.15       # 15% of data for validation

# Paths
paths:
  teacher_model: "../model/clap_audio_model.onnx"  # INPUT: CLAP teacher model (~268MB)
  mel_cache: "/Volumes/audiomuse/student_clap_cache/mel_spectrograms.db"  # SQLite DB (mel specs + embeddings)
  checkpoints: "./checkpoints"
  logs: "./logs"
  final_model: "./models/student_clap_audio.onnx"  # OUTPUT: trained student model (~11MB)

# Dataset configuration - LOCAL FMA
dataset:
  fma_path: "/Users/guidocolangiuli/Music/FMA-JAMENDO"  # Path to local FMA and JAMENADO audio files
  sample_size: 0  # Total number of songs to use (null = all songs)

# Logging
logging:
  level: "INFO"
  log_every: 10
