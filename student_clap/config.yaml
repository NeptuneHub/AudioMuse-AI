# Student CLAP Configuration

# Database connection (PostgreSQL)
database:
  host: ${DB_HOST}
  port: 5432
  database: ${DB_NAME}
  user: ${DB_USER}
  password: ${DB_PASSWORD}

# Jellyfin connection (for audio download)
jellyfin:
  url: ${JELLYFIN_URL}
  user_id: ${JELLYFIN_USER_ID}
  token: ${JELLYFIN_TOKEN}

# Audio processing (MUST match CLAP preprocessing)
audio:
  sample_rate: 48000           # Sample rate for audio loading
  segment_length: 480000       # 10 seconds at 48kHz (CRITICAL: match teacher)
  hop_length: 240000           # 5 seconds hop, 50% overlap (CRITICAL: match teacher)
  
  # Mel-spectrogram parameters (for student model input)
  n_mels: 128                  # Number of mel bands
  n_fft: 2048                  # FFT window size
  hop_length_stft: 480         # STFT hop length
  fmin: 0                      # Minimum frequency
  fmax: 14000                  # Maximum frequency

# Model architecture (TinyCLAP-inspired)
model:
  embedding_dim: 512           # MUST be 512 for CLAP compatibility
  
  # CNN stem
  cnn_channels: [32, 64, 128]
  
  # Transformer
  transformer_layers: 2
  attention_heads: 4
  hidden_dim: 256
  
  dropout: 0.1

# Training hyperparameters
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 0.0001
  epochs: 15
  
  optimizer: "adam"
  weight_decay: 0.00001
  grad_clip: 1.0
  
  # Loss weights
  mse_weight: 0.6
  cosine_weight: 0.4
  
  # Checkpointing
  save_every: 1                # Save checkpoint every epoch
  early_stopping_patience: 10  # Stop if no improvement for N epochs
  
  # Validation
  validation_split: 0.15       # 15% of data for validation

# Paths
paths:
  audio_cache: "./cache/audio"
  max_cache_size_gb: 2         # Keep only 2GB in audio cache
  mel_cache: "./cache/mel_spectrograms.db"
  max_mel_cache_size_gb: 110   # Max mel cache size (stops caching when reached, uses only cached songs)
  checkpoints: "./checkpoints"
  logs: "./logs"
  final_model: "./models/student_clap_audio.onnx"

# Dataset sampling - THIS IS WHERE YOU SET THE 10,000 SONGS!
dataset:
  sample_size: 10000  # ‚≠ê TOTAL NUMBER OF SONGS TO USE (10k = fast training)
  balanced_genres: [  # Sample equally from these genres
    'rock', 'pop', 'alternative', 'indie', 'electronic', 'jazz', 'metal', 'classic rock', 'soul',
    'indie rock', 'electronica', 'folk', 'punk', 'blues', 'hard rock', 'ambient', 'acoustic',
    'experimental', 'Hip-Hop', 'country', 'funk', 'electro', 'heavy metal', 'Progressive rock',
    'rnb', 'indie pop', 'House'
  ]

# Logging
logging:
  level: "INFO"
  log_every: 10
