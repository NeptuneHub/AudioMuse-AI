"""
Student CLAP Training Script

Main entry point for training the lightweight student CLAP model.
Implements real ONNX-based training using PyTorch with knowledge distillation
from existing CLAP embeddings stored in the database.
"""

import os
import sys
import yaml
import logging
import argparse
import numpy as np
import torch
import time
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv

# Load .env file if it exists
load_dotenv()

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from student_clap.data.dataset import StudentCLAPDataset
from student_clap.models.student_onnx_model import StudentCLAPTrainer
from student_clap.training.evaluation import evaluate_embeddings, print_evaluation_report

logger = logging.getLogger(__name__)


def setup_logging(config: dict):
    """Setup logging configuration."""
    log_level = getattr(logging, config['logging']['level'].upper())
    log_dir = Path(config['paths']['logs'])
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'training.log'),
            logging.StreamHandler()
        ]
    )


def expand_env_vars(config: dict) -> dict:
    """Recursively expand environment variables in config."""
    if isinstance(config, dict):
        return {k: expand_env_vars(v) for k, v in config.items()}
    elif isinstance(config, list):
        return [expand_env_vars(item) for item in config]
    elif isinstance(config, str) and config.startswith('${') and config.endswith('}'):
        env_var = config[2:-1]
        return os.environ.get(env_var, config)
    else:
        return config


def train_epoch_real(trainer: StudentCLAPTrainer,
                    dataset: StudentCLAPDataset,
                    config: dict,
                    epoch: int) -> dict:
    """
    Real ONNX-based training epoch using PyTorch with ONNX export.
    
    Uses knowledge distillation from existing CLAP embeddings in database
    to train a lightweight student model following tinyCLAP approach.
    
    Args:
        trainer: Student CLAP trainer with real ONNX model
        dataset: Training dataset
        config: Configuration dict
        epoch: Current epoch number
        
    Returns:
        Dict with epoch metrics
    """
    logger.info(f"ğŸš€ REAL ONNX TRAINING - Epoch {epoch}/{config['training']['epochs']}")
    
    batch_size = config['training']['batch_size']
    
    total_loss = 0.0
    total_mse = 0.0
    total_cosine_sim = 0.0
    num_batches = 0
    num_songs = 0
    
    # Calculate total batches for progress tracking
    total_batches = (len(dataset) + batch_size - 1) // batch_size
    
    epoch_start_time = time.time()
    
    logger.info(f"ğŸ“Š EPOCH {epoch}/{config['training']['epochs']} - Processing {len(dataset)} songs in ~{total_batches} batches")
    
    # Iterate over batches with STREAMING downloads
    for batch_data in tqdm(dataset.iterate_batches_streaming(batch_size, shuffle=True), 
                          desc=f"Epoch {epoch} - Real Training"):
        
        batch_start_time = time.time()
        
        # Prepare batch for training
        batch = {
            'audio_segments': [],
            'teacher_embeddings': [],
            'song_ids': []
        }
        
        for item in batch_data:
            # Get audio segments for this song (already segmented by dataset)
            audio_segments = item['audio_segments']
            batch['audio_segments'].append(audio_segments)
            batch['teacher_embeddings'].append(item['teacher_embedding'])
            batch['song_ids'].append(item['item_id'])
        
        # ğŸ§  REAL TRAINING STEP
        logger.info(f"ğŸ”¥ BATCH {num_batches + 1}/{total_batches} (EPOCH {epoch}/{config['training']['epochs']}): Training on {len(batch_data)} songs...")
        
        try:
            # Forward pass, loss computation, and backward pass
            step_metrics = trainer.train_step(batch)
            
            # Log detailed metrics
            logger.info(f"   âœ… Forward pass through student CNN + Transformer")
            logger.info(f"   ğŸ“Š Loss: {step_metrics['total_loss']:.6f}")
            logger.info(f"      â””â”€ MSE Loss: {step_metrics['mse_loss']:.6f}")
            logger.info(f"      â””â”€ Cosine Loss: {step_metrics['cosine_loss']:.6f}")
            logger.info(f"   ğŸ¯ Cosine Similarity: {step_metrics['mean_cosine_sim']:.4f} (min: {step_metrics['min_cosine_sim']:.4f}, max: {step_metrics['max_cosine_sim']:.4f})")
            
            # Accumulate metrics
            total_loss += step_metrics['total_loss']
            total_mse += step_metrics['mse_loss']
            total_cosine_sim += step_metrics['mean_cosine_sim']
            num_batches += 1
            num_songs += len(batch_data)
            
            batch_time = time.time() - batch_start_time
            epoch_progress = (num_batches + 1) / total_batches * 100
            total_progress = ((epoch - 1) + (num_batches + 1) / total_batches) / config['training']['epochs'] * 100
            
            # Estimate time remaining
            elapsed_time = time.time() - epoch_start_time
            if num_batches > 0:
                avg_batch_time = elapsed_time / (num_batches + 1)
                eta_epoch = avg_batch_time * (total_batches - num_batches - 1)
                logger.info(f"   â±ï¸ Batch: {batch_time:.1f}s ({batch_time/len(batch_data):.1f}s/song)")
                logger.info(f"   ğŸ“ˆ Progress: {epoch_progress:.1f}% epoch, {total_progress:.1f}% total (ETA: {eta_epoch/60:.1f}min)")
            else:
                logger.info(f"   â±ï¸ Batch time: {batch_time:.1f}s ({batch_time/len(batch_data):.1f}s/song)")
            
        except Exception as e:
            logger.error(f"âŒ Training failed on batch {num_batches + 1}: {e}")
            continue
        
        logger.info(f"â”€" * 60)
    
    # Update learning rate scheduler
    trainer.scheduler.step()
    current_lr = trainer.optimizer.param_groups[0]['lr']
    
    # Compute averages
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    avg_mse = total_mse / num_batches if num_batches > 0 else 0.0
    avg_cosine_sim = total_cosine_sim / num_batches if num_batches > 0 else 0.0
    
    epoch_time = time.time() - epoch_start_time
    
    logger.info(f"ğŸ¯ EPOCH {epoch}/{config['training']['epochs']} COMPLETE:")
    logger.info(f"   ğŸ“ˆ Average Loss: {avg_loss:.6f}")
    logger.info(f"   ğŸ“Š Average MSE: {avg_mse:.6f}")
    logger.info(f"   ğŸ¯ Average Cosine Sim: {avg_cosine_sim:.4f}")
    logger.info(f"   ğŸ“š Songs processed: {num_songs}/{len(dataset)} ({num_batches}/{total_batches} batches)")
    logger.info(f"   â±ï¸ Epoch time: {epoch_time/60:.1f}min ({epoch_time/num_songs:.1f}s/song)")
    logger.info(f"   ğŸ“– Learning rate: {current_lr:.2e}")
    
    # Training progress summary
    training_progress = epoch / config['training']['epochs'] * 100
    logger.info(f"ğŸš€ OVERALL TRAINING PROGRESS: {training_progress:.1f}% ({epoch}/{config['training']['epochs']} epochs)")
    
    return {
        'epoch': epoch,
        'avg_loss': avg_loss,
        'avg_mse': avg_mse,
        'avg_cosine_sim': avg_cosine_sim,
        'num_batches': num_batches,
        'num_songs': num_songs,
        'epoch_time': epoch_time,
        'learning_rate': current_lr
    }


def validate_real(trainer: StudentCLAPTrainer,
                 dataset: StudentCLAPDataset,
                 config: dict) -> dict:
    """
    Real validation using trained student model.
    
    Args:
        trainer: Student CLAP trainer with trained model
        dataset: Validation dataset
        config: Configuration dict
        
    Returns:
        Dict with validation metrics
    """
    logger.info("ğŸ” Running REAL validation...")
    
    trainer.model.eval()
    
    # Collect embeddings
    student_embeddings_list = []
    teacher_embeddings_list = []
    song_ids = []
    
    with torch.no_grad():
        for batch_data in tqdm(dataset.iterate_batches_streaming(config['training']['batch_size'], shuffle=False),
                              desc="Validation"):
            
            # Prepare batch
            batch = {
                'audio_segments': [],
                'teacher_embeddings': [],
                'song_ids': []
            }
            
            for item in batch_data:
                audio_segments = item['audio_segments']
                batch['audio_segments'].append(audio_segments)
                batch['teacher_embeddings'].append(item['teacher_embedding'])
                batch['song_ids'].append(item['item_id'])
            
            # Forward pass without gradients
            student_embeddings = []
            for i, audio_segments in enumerate(batch['audio_segments']):
                # Convert to tensor
                if not isinstance(audio_segments, torch.Tensor):
                    audio_segments = torch.tensor(audio_segments, dtype=torch.float32, device=trainer.device)
                
                # Get averaged embedding
                avg_embedding = trainer.model.process_audio_segments(audio_segments)
                student_embeddings.append(avg_embedding.cpu().numpy())
            
            # Stack and store
            student_batch = np.vstack(student_embeddings)
            teacher_batch = np.stack(batch['teacher_embeddings'])
            
            student_embeddings_list.append(student_batch)
            teacher_embeddings_list.append(teacher_batch)
            song_ids.extend(batch['song_ids'])
    
    # Concatenate all embeddings
    student_all = np.vstack(student_embeddings_list)
    teacher_all = np.vstack(teacher_embeddings_list)
    
    # Evaluate
    metrics = evaluate_embeddings(student_all, teacher_all)
    metrics['num_songs'] = len(song_ids)
    
    return metrics


def train(config_path: str, resume: str = None):
    """
    Main training loop with real ONNX implementation.
    
    Args:
        config_path: Path to config file
        resume: Path to checkpoint to resume from
    """
    # Load config
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Expand environment variables
    config = expand_env_vars(config)
    
    # Setup logging
    setup_logging(config)
    
    logger.info("=" * 60)
    logger.info("ğŸš€ Student CLAP REAL ONNX Training")
    logger.info("=" * 60)
    
    # Initialize trainer with real ONNX model
    logger.info("\nğŸ—ï¸ Building Student CLAP model...")
    trainer = StudentCLAPTrainer(config)
    
    # Print model info
    model_info = trainer.model.count_parameters()
    logger.info(f"\nğŸ“Š Model Architecture:")
    logger.info(f"   Total parameters: {model_info['total_parameters']:,}")
    logger.info(f"   Trainable parameters: {model_info['trainable_parameters']:,}")
    logger.info(f"   Estimated size: {model_info['estimated_size_mb']:.1f} MB")
    logger.info(f"   Device: {trainer.device}")
    
    # Create datasets
    logger.info("\nğŸ“ Creating datasets...")
    train_dataset = StudentCLAPDataset(config, split='train', 
                                       validation_split=config['training']['validation_split'])
    val_dataset = StudentCLAPDataset(config, split='val',
                                     validation_split=config['training']['validation_split'])
    
    logger.info(f"  Train: {len(train_dataset)} samples")
    logger.info(f"  Val:   {len(val_dataset)} samples")
    
    # Print dataset stats
    train_stats = train_dataset.get_dataset_stats()
    logger.info(f"\nğŸ“ˆ Dataset statistics:")
    for key, value in train_stats.items():
        logger.info(f"  {key}: {value}")
    
    # Create checkpoint directory
    checkpoint_dir = Path(config['paths']['checkpoints'])
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    # Resume from checkpoint if specified
    start_epoch = 1
    if resume:
        logger.info(f"\nâ®ï¸ Resuming from checkpoint: {resume}")
        # In a full implementation, load checkpoint here
        # trainer.load_checkpoint(resume)
    
    # Training loop
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ Starting REAL ONNX Training...")
    logger.info("   ğŸ“š Using existing CLAP embeddings from database")
    logger.info("   ğŸ—ï¸ Knowledge distillation: Teacher (268MB) â†’ Student (~20-40MB)")
    logger.info("   ğŸµ Music-specialized compression following tinyCLAP")
    logger.info("=" * 60)
    
    best_val_cosine = 0.0
    patience_counter = 0
    training_start_time = time.time()
    
    for epoch in range(start_epoch, config['training']['epochs'] + 1):
        # Train epoch with REAL implementation
        train_metrics = train_epoch_real(trainer, train_dataset, config, epoch)
        
        # Validate every few epochs
        if epoch % 5 == 0 or epoch == 1:
            val_metrics = validate_real(trainer, val_dataset, config)
            print_evaluation_report(val_metrics, f"Validation - Epoch {epoch}")
            
            # Check for improvement (use cosine similarity as main metric)
            val_cosine = val_metrics['cosine_similarity']
            if val_cosine > best_val_cosine:
                best_val_cosine = val_cosine
                patience_counter = 0
                logger.info(f"âœ“ New best validation cosine similarity: {best_val_cosine:.4f}")
                
                # Save best checkpoint
                best_checkpoint_path = checkpoint_dir / f"best_model_epoch_{epoch}.pth"
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': trainer.model.state_dict(),
                    'optimizer_state_dict': trainer.optimizer.state_dict(),
                    'scheduler_state_dict': trainer.scheduler.state_dict(),
                    'val_cosine_sim': val_cosine,
                    'config': config
                }, best_checkpoint_path)
                logger.info(f"  ğŸ’¾ Saved best checkpoint: {best_checkpoint_path}")
                
                # Export to ONNX
                onnx_path = checkpoint_dir / f"best_model_epoch_{epoch}.onnx"
                trainer.export_to_onnx(str(onnx_path))
                
            else:
                patience_counter += 1
                logger.info(f"No improvement ({patience_counter}/{config['training']['early_stopping_patience']})")
        
        # Save regular checkpoint
        if epoch % config['training']['save_every'] == 0:
            checkpoint_path = checkpoint_dir / f"epoch_{epoch}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': trainer.model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'scheduler_state_dict': trainer.scheduler.state_dict(),
                'train_metrics': train_metrics,
                'config': config
            }, checkpoint_path)
            logger.info(f"ğŸ’¾ Saved checkpoint: {checkpoint_path}")
        
        # Early stopping
        if patience_counter >= config['training']['early_stopping_patience']:
            logger.info(f"\nâ¹ï¸ Early stopping triggered after {epoch} epochs")
            break
    
    # Training complete
    total_training_time = time.time() - training_start_time
    logger.info(f"\nğŸ‰ Training complete!")
    logger.info(f"   â±ï¸ Total training time: {total_training_time/3600:.1f} hours")
    logger.info(f"   ğŸ† Best validation cosine similarity: {best_val_cosine:.4f}")
    
    # Final model export
    final_model_path = Path(config['paths']['final_model'])
    final_model_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Save final PyTorch model
    final_pth_path = final_model_path.with_suffix('.pth')
    torch.save({
        'model_state_dict': trainer.model.state_dict(),
        'config': config,
        'best_val_cosine_sim': best_val_cosine,
        'total_epochs': epoch
    }, final_pth_path)
    
    # Export final ONNX model
    trainer.export_to_onnx(str(final_model_path))
    logger.info(f"ğŸ¯ Final ONNX model: {final_model_path}")
    logger.info(f"ğŸ¯ Final PyTorch model: {final_pth_path}")
    
    # Cleanup
    train_dataset.close()
    val_dataset.close()
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… REAL ONNX TRAINING COMPLETE!")
    logger.info("   ğŸµ Student model ready for production deployment")
    logger.info("   ğŸ“¦ Drop-in replacement for existing CLAP audio encoder")
    logger.info("   ğŸš€ Expected 5-10x size reduction, 2-5x speed improvement")
    logger.info("=" * 60)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train Student CLAP model with REAL ONNX implementation')
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    args = parser.parse_args()
    
    try:
        train(args.config, args.resume)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Training interrupted by user")
        sys.exit(0)
    except Exception as e:
        logging.error(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)