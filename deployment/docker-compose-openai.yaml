version: '3.8'
services:
  # Redis service for RQ (task queue)
  redis:
    image: redis:7-alpine
    container_name: audiomuse-redis
    ports:
      - "6379:6379" # Expose Redis port to the host
    volumes:
      - redis-data:/data # Persistent storage for Redis data
    restart: unless-stopped

  # PostgreSQL database service
  postgres:
    image: postgres:15-alpine
    container_name: audiomuse-postgres
    environment:
      POSTGRES_USER: "audiomuse"
      POSTGRES_PASSWORD: "audiomusepassword"
      POSTGRES_DB: "audiomusedb"
    ports:
      - "5432:5432" # Expose PostgreSQL port to the host
    volumes:
      - postgres-data:/var/lib/postgresql/data # Persistent storage for PostgreSQL data
    restart: unless-stopped

  # AudioMuse-AI Flask application service
  audiomuse-ai-flask:
    image: ghcr.io/neptunehub/audiomuse-ai:latest # Reflects deployment.yaml
    container_name: audiomuse-ai-flask-app
    ports:
      - "8000:8000" # Map host port 8000 to container port 8000
    environment:
      SERVICE_TYPE: "flask" # Tells the container to run the Flask app
      MEDIASERVER_TYPE: "emby" # Specify the media server type
      EMBY_USER_ID: "xxxxb4fb-4e2d-4c3d-xxxx-xxxxxxxxxxxx" # Example: xxxxb4fb-4e2d-4c3d-xxxx-xxxxxxxxxxxx
      EMBY_TOKEN: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Example: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
      EMBY_URL: "http://embyserver:80"
      # DATABASE_URL is now constructed by config.py from the following:
      POSTGRES_USER: "audiomuse"
      POSTGRES_PASSWORD: "audiomusepassword"
      POSTGRES_DB: "audiomusedb"
      POSTGRES_HOST: "postgres" # Service name of the postgres container
      POSTGRES_PORT: "5432"
      REDIS_URL: "redis://redis:6379/0" # Connects to the 'redis' service
      AI_MODEL_PROVIDER: "OPENAI"
      OPENAI_API_KEY: "YOUR_OPENAI_API_KEY_HERE" # From openai-api-credentials secret. For DMR or self hosted LLMs, add a dummy key.
      OPENAI_MODEL_NAME: "ai/qwen3:0.6B-Q4_0" # Specify the OpenAI model to use
      OPENAI_BASE_URL: "http://172.17.0.1:12434/engines/llama.cpp/v1/chat/completions" # OpenAI API URL
      OPENAI_API_TOKENS: "1000" # Max tokens for OpenAI API calls
      TEMP_DIR: "/app/temp_audio"
    #volumes:
    #  - temp-audio-flask:/app/temp_audio # Volume for temporary audio files
    tmpfs:  # Use tmpfs for temporary audio files instead of a volume
      - /app/temp_audio:rw
    depends_on:
      - redis
      - postgres
    restart: unless-stopped
    models:
      - llm

  # AudioMuse-AI RQ Worker service
  audiomuse-ai-worker:
    image: ghcr.io/neptunehub/audiomuse-ai:latest # Reflects deployment.yaml
    container_name: audiomuse-ai-worker-instance
    environment:
      SERVICE_TYPE: "worker" # Tells the container to run the RQ worker
      MEDIASERVER_TYPE: "emby" # Specify the media server type
      EMBY_USER_ID: "xxxxb4fb-4e2d-4c3d-xxxx-xxxxxxxxxxxx" # Example: xxxxb4fb-4e2d-4c3d-xxxx-xxxxxxxxxxxx
      EMBY_TOKEN: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Example: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
      EMBY_URL: "http://embyserver:80"
      # DATABASE_URL is now constructed by config.py from the following:
      POSTGRES_USER: "audiomuse"
      POSTGRES_PASSWORD: "audiomusepassword"
      POSTGRES_DB: "audiomusedb"
      POSTGRES_HOST: "postgres" # Service name of the postgres container
      POSTGRES_PORT: "5432"
      REDIS_URL: "redis://redis:6379/0" # Connects to the 'redis' service
      AI_MODEL_PROVIDER: "OPENAI"
      OPENAI_API_KEY: "YOUR_OPENAI_API_KEY_HERE" # From openai-api-credentials secret. For DMR or self hosted LLMs, add a dummy key.
      OPENAI_MODEL_NAME: "ai/qwen3:0.6B-Q4_0" # Specify the OpenAI model to use
      OPENAI_BASE_URL: "http://172.17.0.1:12434/engines/llama.cpp/v1/chat/completions" # OpenAI API URL
      OPENAI_API_TOKENS: "1000" # Max tokens for OpenAI API calls
      TEMP_DIR: "/app/temp_audio"
    #volumes:
    #  - temp-audio-worker:/app/temp_audio # Volume for temporary audio files
    tmpfs:  # Use tmpfs for temporary audio files instead of a volume
      - /app/temp_audio:rw
    depends_on:
      - redis
      - postgres
    restart: unless-stopped
    models:
      - llm
# Docker Model Runner uses the same API endpoints as OpenAI. https://docs.docker.com/ai/model-runner/
models:
  llm:
    model: ai/qwen3:0.6B-Q4_0 # very lightweight local model for testing. Change as needed. and make sure it maches OPENAI_MODEL_NAME

# Define volumes for persistent data and temporary files
volumes:
  redis-data:
  postgres-data:
  temp-audio-flask: # Volume for Flask app's temporary audio
  temp-audio-worker: # Volume for Worker's temporary audio
