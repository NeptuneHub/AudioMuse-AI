# AudioMuse-AI Deployment Configuration
#
# WORKER LOCAL GPU TEMPLATE - Build and run a GPU-enabled worker for heavy tasks like analysis and clustering.
# This worker connects to a remote server running AudioMuse-AI Flask application and databases.
# Builds the image locally with GPU support.

services:
  # AudioMuse-AI Worker service (GPU-enabled, locally built)
  audiomuse-ai-worker:
    image: audiomuse-ai:worker-gpu
    build:
      context: ../       # Build from project root (one level up from deployment/)
      dockerfile: Dockerfile
      args:  # GPU build args
        BASE_IMAGE: nvidia/cuda:12.8.1-cudnn-runtime-ubuntu22.04
    container_name: audiomuse-ai-worker-instance
    ports:
      - "${WORKER_PORT:-8029}:8000"  # Expose worker API
    environment:
      SERVICE_TYPE: "worker"
      MEDIASERVER_TYPE: "${MEDIASERVER_TYPE:-jellyfin}"
      JELLYFIN_USER_ID: "${JELLYFIN_USER_ID}"
      JELLYFIN_TOKEN: "${JELLYFIN_TOKEN}"
      JELLYFIN_URL: "${JELLYFIN_URL}"
      NAVIDROME_URL: "${NAVIDROME_URL}"
      NAVIDROME_USER: "${NAVIDROME_USER}"
      NAVIDROME_PASSWORD: "${NAVIDROME_PASSWORD}"
      LYRION_URL: "${LYRION_URL}"
      EMBY_URL: "${EMBY_URL}"
      EMBY_USER_ID: "${EMBY_USER_ID}"
      EMBY_TOKEN: "${EMBY_TOKEN}"
      POSTGRES_USER: ${POSTGRES_USER:-audiomuse}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-audiomusepassword}
      POSTGRES_DB: ${POSTGRES_DB:-audiomusedb}
      POSTGRES_HOST: "${WORKER_POSTGRES_HOST:-postgres}" # Replace via WORKER_POSTGRES_HOST in .env when running remotely
      POSTGRES_PORT: "${POSTGRES_PORT:-5432}"
      REDIS_URL: "${WORKER_REDIS_URL:-redis://redis:6379/0}"  # Set WORKER_REDIS_URL in .env for remote connections
      AI_MODEL_PROVIDER: "${AI_MODEL_PROVIDER}"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
      OPENAI_SERVER_URL: "${OPENAI_SERVER_URL}"
      OPENAI_MODEL_NAME: "${OPENAI_MODEL_NAME}"
      GEMINI_API_KEY: "${GEMINI_API_KEY}"
      MISTRAL_API_KEY: "${MISTRAL_API_KEY}"
      CLAP_ENABLED: "${CLAP_ENABLED:-true}" # Enable CLAP text search (set to false for slower systems)
      TEMP_DIR: "/app/temp_audio"
      USE_GPU_CLUSTERING: "${USE_GPU_CLUSTERING:-true}"
      CLAP_PYTHON_MULTITHREADS: "${CLAP_PYTHON_MULTITHREADS:-false}"
    volumes:
      - temp-audio-worker:/app/temp_audio
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

volumes:
  temp-audio-worker:
